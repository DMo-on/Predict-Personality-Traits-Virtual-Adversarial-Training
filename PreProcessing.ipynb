{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PreProcessing.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"U4D9CzDRIIlS"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4LLwHhyJOdg","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e23a2526-8e38-4e33-851c-2b88216b3878"},"source":["! pip install regex "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A3VogfXBU9cL","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"ae912ef0-3b39-4b29-8474-b91f53b524e0"},"source":["import pandas as pd\n","data = pd.read_csv(\"gdrive/My Drive/AI-project/MyPersonality.csv\")\n","#data = pd.read_csv(\"gdrive/My Drive/AI-project/reddit.tsv\",sep='\\t')\n","print(data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1013000, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hne6nBvOdpRV"},"source":["#clean dataset\n","import sys\n","import regex as re\n","import tokenize\n","import pandas as pd\n","import os\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import unicodedata \n","from string import punctuation\n","import fileinput\n","from itertools import groupby, product\n","from datetime import datetime\n","debut=datetime.now()\n","\n","tokens_twitter = {}\n","f = open(\"gdrive/My Drive/AI-project/glove.twitter.27B.25d.txt\", 'r', encoding=\"utf8\")\n","key=-1\n","for line in f:\n","    key+=1\n","    (val, vector) = line.split(\" \", 1)\n","    tokens_twitter[int(key)] = val\n","\n","\n","\n","\n","\n","FLAGS = re.MULTILINE | re.DOTALL\n","from nltk.tokenize import TweetTokenizer\n","tokenizer = TweetTokenizer()\n","def hashtag(text): #hashtag\n","    text = text.group()\n","    hashtag_body = text[1:]\n","    if hashtag_body.isupper():\n","        result = \"<hashtag> {} <allcaps>\".format(hashtag_body)\n","    else:\n","        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n","    return result\n","\n","def allcaps(text): #mot tout en maj\n","    text = text.group()\n","    return text.lower() + \" <allcaps>\"\n","\n","\n","def load_dict_emojis():\n","    \n","    return {\n","        \":‑)\":\"<smile>\",\n","        \"=]]\":\"<smile>\",\n","        \":]\":\"<smile>\",\n","        \":}\":\"<smile>\",\n","        \":o)\":\"<neutralface>\",\n","        \"=]\":\"<smile>\",\n","        \"=)\":\"<smile>\",\n","        \":D\":\"<smile>\",\n","        \"xD\":\"<lolface>\",\n","        \"XD\":\"<lolface>\",\n","        'xd':\"<lolface>\",\n","        \":[\":\"<sadface>\",\n","        \"=[[\":\"<sadface>\",\n","        \":{\":\"<sadface>\",\n","        \":@\":\"<sadface>\",\n","        \":'-(\":\"<sadface>\",\n","        \"^^\":\"<smile>\",\n","        \"¬_¬\":\"<neutralface>\",\n","        \"o_O\":\"<neutralface>\",\n","        \"o_o\":\"<neutralface>\",\n","        \"¤_¤\":\"<smile>\",\n","        \"^o^\":\"<smile>\",\n","        \"^_^\":\"<smile>\"\n","        \n","        }\n","    \n","    \n","def emoji_to_text(text):\n","    emojis = load_dict_emojis()  \n","    words = text.split()\n","    reformed = [emojis[word] if word in emojis else word for word in words]\n","    return \" \".join(reformed)\n","\n","\n","\n","def remove_punctuation(text):\n","    punctuation=['\"',\"'\", \"’\" ]\n","    return ''.join(c for c in text if c not in punctuation)\n","\n","\n","\n","def remove_consecutive_dups(s):\n","    return re.sub(r'(?i)(.)\\1+', r'\\1', s)\n","\n","def all_consecutive_duplicates_edits(word, max_repeat=float('inf')):\n","    chars = [[c*i for i in range(min(len(list(dups)), max_repeat), 0, -1)]\n","             for c, dups in groupby(word)]   \n","    elong=False   \n","    for c in chars:\n","      if len(c)>2:\n","        elong=True\n","\n","    return (map(''.join, product(*chars)),elong)\n","\n","\n","        \n","        \n","\n","def clean(text):\n","    # Different regex parts for smiley faces\n","    eyes = r\"[8:=;]\"\n","    nose = r\"['`\\-o]?\"\n","    eyeleft = r\"[>*^o\\-¤¬]\" \n","    mouth = r\"[_o\\.]?\"\n","\n","    # function so code less repetitive\n","    def re_sub(pattern, repl):\n","        return re.sub(pattern, repl, text, flags=FLAGS)\n","    \n","    \n","    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\") #url\n","    text = text.replace(\"<PERSON>\", \"<user>\")\n","    text = text.replace(\"<URL>\", \"<url>\")\n","    \n","    text = re_sub(r\"([A-Z]){2,}\", allcaps) #mettre tout en minuscule\n","    text=text.lower()    \n","    text = re_sub(r\"\\.[ ]+\\.\", \"..\") \n","    text = text.replace(\"‘\", \"'\")\n","\n","    \n","    text = re.sub(r\"-{2,}\", r\"-\", text) \n","    text = re.sub(r\"_{2,}\", r\"_\", text)\n","\n","    text = re.sub(r\"([.]){4,}\", r\"… <repeat>\", text)\n","    text = re.sub(r\"([.]){2,3}\", r\"…\", text)\n","    \n","    \n","    text = text.replace(\"“\", '\"')\n","    text = text.replace(\"”\", '\"')\n","    text = text.replace(\"`\", \"'\")\n","    text = text.replace(\"¸¸\", \",\")\n","    text = text.replace(\"pl0x\", \"plox\")\n","    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n","    \n","    text = text.replace(\"*propname*\", \"<user>\")\n","    \n","    \n","    text = text.replace(\"*propname <allcaps>*\", \"<user>\")\n","    text = re_sub(r\"@\\w+\", \"<user>\")\n","    emojis = load_dict_emojis()  \n","    text = emoji_to_text(text) \n","    text = re_sub(r\"{}{}[)dD3\\]]+|[\\[(dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n","\n","    text = re_sub(r\"[>*\\^\\-¬][_o]{1}[<*\\^\\-¬]\", \"<smile>\")\n","    text = re_sub(r\"[o¤0][_]{1}[o0¤]\", \"<smile>\")\n","    text = re_sub(r\"\\^\\^\", \"<smile>#\")\n","    eyeleft = r\"[>*\\^o\\-¤¬]\" \n","    mouth = r\"[_o\\.]?\"\n","\n","\n","    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n","    text = re_sub(r\"x[d]+|[d]+x\", \"<lolface>\")\n","    text = re_sub(r\":[o]+|[o]+:\", \"<neutralface>\")\n","    text = re_sub(r\":[}]+|[{]+:\", \"<smile>\")\n","    text = re_sub(r\":[{]+|[}]+:\", \"<sadface>\")\n","    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n","    text = re_sub(r\"{}{}@+|@+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n","    text = re_sub(r\"{}{}[+|]+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n","\n","    text = re_sub(r\"/\",\" / \")\n","    \n","    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n","    text = re_sub(r\"<3\",\"<heart>\")\n","    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n","    text = re_sub(r\"#\\S+\", hashtag)\n","    text = re_sub(r'([a-z])_', r'\\1 ')  \n","    text = re_sub(r'_([a-z])', r' \\1')  \n","    text = re_sub(r'([^a-z])\\1+', r'\\1')      \n","    text = re_sub(r\"a[_*\\.\\-]s\", \"ass\")\n","    text = re_sub(\"( |^)f[_*\\.\\-]( |$)\", \" fuck \")\n","    text = re_sub(\"( |^)[_*\\.\\-]up( |$|[^a-z])\", \" fuck up \")\n","\n","    text = text.replace(\"t-shirt\", \"tshirt\")      \n","                  \n","#    words = enchant.Dict(\"en\")\n","#    is_known_word = words.check # a changer avec notre dictionnary\n","    \n","    from nltk.tokenize import TweetTokenizer\n","    tokenizer = TweetTokenizer()\n","    regex = re.compile(r\"(.)\\1{2}\")\n","\n","    if regex.search(text):\n","      text_shorten=[]\n","      for t in tokenizer.tokenize(text):\n","        if (t not in tokens_twitter.values()) :\n","          if all_consecutive_duplicates_edits(t)[1]:\n","            text_shorten.append(remove_consecutive_dups(t)+' <elong> ')\n","          else:\n","            text_shorten.append(t)\n","        else:    \n","          text_shorten.append(t)\n","      text=' '.join(text_shorten)\n","\n","\n","\n","    text = re.sub(r'(\\w+ )(\\1)+', r'\\1<repeat> ', text) #les mots qui se repete ex: bye bye\n","    text = re.sub(r'\\W(\\w+)(\\1)+\\W', r' \\1 <repeat> ', text)\n","    text = re.sub(r'^(\\w+)(\\1)+', r' \\1 <repeat> ', text)\n","    text = re.sub(r'(\\w+ )(\\1)+', r'\\1<repeat> ', text) #les mots qui se repete ex: bye bye\n","\n","    string=''\n","    for token in text.split(' '):\n","      m=re.search('(.*?)(([a-z]\\.){2,})(.*)', token)\n","      if m:\n","        replacement=''.join(m.group(2).split('.'))\n","        token=m.group(1)+replacement+m.group(4)\n","        \n","      string+=token+' '  \n","    text=string\n","\n","    string=''\n","    for token in text.split(' '):\n","      m=re.search('(.*?)(([a-z]-){2,})(.*)', token)\n","      if m:\n","        replacement=''.join(m.group(2).split('-'))\n","        token=m.group(1)+replacement+m.group(4)\n","      string+=token+' ' \n","      text=string\n","\n","    \n","    text = re.sub(r'([A-Za-z])-([A-Za-z])', lambda m: '%s %s' % (m.groups()), text)#ex: half-done=>half done\n","    text = re.sub(r'([A-Za-z])_([A-Za-z])', lambda m: '%s %s' % (m.groups()), text)#ex: half_done=>half done\n","\n","\n","    text = re.sub(r'([A-Za-z]);([A-Za-z]*)', r'\\1 ;\\2', text)#ex:  do;e => do ;e\n","    text = re.sub(r'([A-Za-z]):([A-Za-z]*)', r'\\1 :\\2', text)#ex:  do:e => do :e\n","\n","\n","    text = text.replace(\"'\", \"\")\n","    text = text.replace(\"’\", \"\")\n","\n","    text = re.sub(r'([A-Za-z])\\.([A-Za-z])', r'\\1. \\2', text)#ex: elle.lui=> elle. lui\n","\n","    text = re.sub(r\"\\b(\\S*?)(.)\\2{2,}(\\S*?)\", r\"\\1\\2\",text)\n","\n","    text = text.replace(\"<-\", \" \")\n","    text = text.replace(\"->\", \" \")\n","    text = text.replace(r\" > < \",\" >< \")\n","    text = text.replace(r\" :\\ \",\" <sadface> \")\n","    text = text.replace(r\" t t \",\" <sadface> \")\n","    text = text.replace(r\" n n \",\" <smile> \")\n","    text = text.replace(r\" n <repeat> \",\" <smile> \")\n","    text = text.replace(r\" m <repeat> \",\" mmm \")\n","    text = text.replace(r\" w <number> t \",\" woot \")\n","    text = text.replace(r\" l <repeat>\",\" ll\")\n","    text = text.replace(r\" <allcaps> s \",\" s <allcaps> \")\n","    text = text.replace(r\" <allcaps> t \",\" t <allcaps> \")\n","    text = text.replace(\"<lb>\", \" \")  \n","    text = re_sub(r\"\\s+\", \" \")\n","    return text\n","\n","\n","\n","#Clean dataset \n","pd.set_option('display.max_columns', None)\n","i=-1\n","for s in data['text']: #selftext\n","    i+=1\n","    print(i)\n","    s= clean(s)\n","    data.at[i, 'text']=s\n","\n","\n","\n","#Emoji to text \n","from nltk.tokenize import TweetTokenizer\n","tokenizer = TweetTokenizer()\n","import demoji\n","demoji.download_codes()\n","for s in data[\"selftext\"]:\n","    i+=1\n","    tokens= tokenizer.tokenize(s)\n","    chaine=''\n","    for t in tokens:\n","      corr=t\n","      if demoji.findall(t)!={}:\n","        exclude = set(string.punctuation)\n","        emojis=demoji.findall(t)\n","        if(t in emojis):\n","          corr = ''.join(ch for ch in emojis[t].lower() if ch not in exclude)\n","          \n","\n","      chaine=chaine+corr+' '\n","           \n","    data.at[i, 'text']=chaine\n","\n","\n","\n","data.to_csv(r'gdrive/My Drive/AI-project/mypersonality_final.csv', index = False) \n","#data.to_csv(r'gdrive/My Drive/AI-project/unlab_reddit.csv', index = False)  \n"],"execution_count":null,"outputs":[]}]}