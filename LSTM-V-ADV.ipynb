{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM-V-ADV.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOhhvlw/QmsfvTvDkSrr8g8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"jk5pOOG3vBy3"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9o5CMfI2vIBP"},"source":["#Requirements\n","!pip install tensorflow==1.5\n","!pip install keras==2.1.5\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m_X1DqFTvDYJ"},"source":["import tensorflow as tf\n","from tensorflow import keras as K\n","import numpy as np\n","import argparse\n","import h5py\n","\n","from progressbar import ProgressBar\n","import os\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import warnings\n","from keras import backend as b\n","from sklearn.metrics import confusion_matrix\n","\n","\n","savepath = 'gdrive/My Drive/AI-project/adv_train/'\n","maxlen=30\n","#initialiser tensorflow\n","init_op = tf.initialize_all_variables()\n","\n","sess = tf.Session()\n","sess.run(init_op)\n","\n","class Network:\n","    def __init__(self, session, dict_weight, lr ,dropout=0.2, lstm_units=256, dense_units=70):\n","\n","        #Creer reseau\n","        self.sess = session\n","        K.backend.set_session(self.sess)\n","\n","        # definir couches\n","        dict_shape = dict_weight.shape\n","        self.emb = K.layers.Embedding(dict_shape[0], dict_shape[1], weights=[dict_weight], trainable=False, name='embedding')\n","        self.drop = K.layers.Dropout(rate=dropout, seed=91, name='dropout')\n","        self.lstm = K.layers.LSTM(lstm_units, stateful=False, return_sequences=False, name='lstm')\n","        self.dense = K.layers.Dense(dense_units, activation='relu', name='dense')\n","        self.p = K.layers.Dense(1, activation='sigmoid', name='p')\n","\n","        # Definir optimisation\n","        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n","\n","\n","    \"\"\"\n","    Function that returns the output of the network and the output of the embedding layer\n","    \"\"\"\n","    def __call__(self, batch, perturbation=None):\n","        embedding = self.emb(batch) \n","        drop = self.drop(embedding)\n","        if (perturbation is not None):\n","            drop += perturbation\n","        lstm = self.lstm(drop)\n","        dense = self.dense(lstm)\n","        return self.p(dense), embedding\n","    \n","    \"\"\"\n","    Function that generates mini-batches\n","    \"\"\"\n","    def get_minibatch(self, x, y, ul, batch_shape=(512, 30)):\n","        x = K.preprocessing.sequence.pad_sequences(x, maxlen=batch_shape[1])\n","        permutations = np.random.permutation( len(y) )\n","        ul_permutations = None\n","        len_ratio = None\n","        if (ul is not None):\n","            ul = K.preprocessing.sequence.pad_sequences(ul, maxlen=batch_shape[1])\n","            ul_permutations = np.random.permutation( len(ul) )\n","            len_ratio = len(ul)/len(y)\n","        for s in range(0, len(y), batch_shape[0]):\n","            perm = permutations[s:s+batch_shape[0]]\n","            minibatch = {'x': x[perm], 'y': y[perm]}\n","            if (ul is not None):\n","                ul_perm = ul_permutations[int(np.floor(len_ratio*s)):int(np.floor(len_ratio*(s+batch_shape[0])))]\n","                minibatch.update( {'ul': np.concatenate((ul[ul_perm], x[perm]), axis=0)} )\n","            yield minibatch\n","\n","    \"\"\"\n","        Function that returns the error and the output of the embedding layer \n","    \"\"\"        \n","    def get_loss(self, batch, labels):\n","        pred, emb = self(batch)\n","        loss = K.losses.binary_crossentropy(labels, pred)\n","        return tf.reduce_mean( loss ), emb\n","    \n","    \"\"\"\n","        Function that calculates and returns the advarsarial error\n","    \"\"\"  \n","    def get_adv_loss(self, batch, labels, loss, emb, p_mult):\n","        gradient = tf.gradients(loss, emb, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n","        p_adv = p_mult * tf.nn.l2_normalize(tf.stop_gradient(gradient), dim=1)\n","        adv_loss = K.losses.binary_crossentropy(labels, self(batch, p_adv)[0])\n","        return tf.reduce_mean( adv_loss )\n","\n","    \"\"\"\n","        Function that calculates and returns the virtual advarsarial error\n","    \"\"\"  \n","    def get_v_adv_loss(self, ul_batch, p_mult, power_iterations=1):\n","        bernoulli = tf.distributions.Bernoulli\n","        prob, emb = self(ul_batch)\n","        prob = tf.clip_by_value(prob, 1e-7, 1.-1e-7)\n","        prob_dist = bernoulli(probs=prob)\n","        # Generation de la perturbation adv virt\n","        d = tf.random_uniform(shape=tf.shape(emb), dtype=tf.float32)\n","        for _ in range( power_iterations ):\n","            d = (0.02) * tf.nn.l2_normalize(d, dim=1)\n","            p_prob = tf.clip_by_value(self(ul_batch, d)[0], 1e-7, 1.-1e-7)\n","            kl = tf.distributions.kl_divergence(prob_dist, bernoulli(probs=p_prob), allow_nan_stats=False)\n","            gradient = tf.gradients(kl, [d], aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)[0]\n","            d = tf.stop_gradient(gradient)\n","        d = p_mult * tf.nn.l2_normalize(d, dim=1)\n","        tf.stop_gradient(prob)\n","        # Calcul de l'erreur\n","        p_prob = tf.clip_by_value(self(ul_batch, d)[0], 1e-7, 1.-1e-7)\n","        v_adv_loss = tf.distributions.kl_divergence(prob_dist, bernoulli(probs=p_prob), allow_nan_stats=False)\n","        return tf.reduce_mean( v_adv_loss )\n","\n","    \"\"\"\n","      Validation\n","    \"\"\"\n","    def validation(self, x, y, trait, batch_shape=(512, 30)):\n","        print( 'Validation...' )\n","        \n","        labels = tf.placeholder(tf.float32, shape=(None, 1), name='validation_labels')\n","        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='validation_batch')\n","\n","        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n","        y_true=labels\n","        y_pred=self(batch)[0]\n","\n","        true_positives = b.sum(b.round(b.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = b.sum(b.round(b.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + b.epsilon())\n","        true_positives = b.sum(b.round(b.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = b.sum(b.round(b.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + b.epsilon())\n","        fscore= 2*((precision*recall)/(precision+recall+b.epsilon()))\n","\n","        fscores = list()\n","        accuracies = list()\n","        minibatch = self.get_minibatch(x, y, ul=None, batch_shape=batch_shape)\n","        for val_batch in minibatch:\n","            fd = {batch: val_batch['x'], labels: val_batch['y'], K.backend.learning_phase(): 0} #test mode\n","            accuracies.append( self.sess.run(accuracy, feed_dict=fd) )\n","            fscores.append(self.sess.run(fscore, feed_dict=fd))\n","        logfile = savepath+'lstm_adv'+trait+'.log'\n","        log_msg = \" val accuracy is {:.3f} (train) -- val f1score is {:.3f}\"\n","        results = open(logfile,\"a\")\n","        results.write('\\n\\n')\n","        results.write(log_msg.format(np.asarray(accuracies).mean(),  np.asarray(fscores).mean())+'\\n\\n')\n","        results.close()\n","        print( \"Average accuracy on validation is {:.3f}\".format(np.asarray(accuracies).mean()) )\n","\n","\n","\n","    \"\"\"\n","       Training \n","    \"\"\"\n","    def train(self,dataset, trait, dict_weight,batch_shape=(512, 30), epochs=25, loss_type='none', p_mult=0.02, init=None, save=None, dropout=0.2, lstm_units=256, dense_units=70,saving=None):\n","        \n","        \n","        print( 'Training...' )\n","        xtrain = np.load( \"{}\".format(dataset)+trait+\"_xtrain.npy\",allow_pickle=True)\n","        ytrain = np.load( \"{}\".format(dataset)+trait+\"_ytrain.npy\" ,allow_pickle=True)\n","        ultrain = np.load( \"{}\".format(dataset)+trait+\"_ultrain.npy\",allow_pickle=True) if (loss_type == 'v_adv') else None\n","        \n","        \n","        # Definition ens val\n","        xval = list()\n","        yval = list()\n","        for _ in range( int(len(ytrain)*0.025) ):\n","            xval.append( xtrain[0] ); xval.append( xtrain[-1] )\n","            yval.append( ytrain[0] ); yval.append( ytrain[-1] )\n","            xtrain = np.delete(xtrain, 0); xtrain = np.delete(xtrain, -1)\n","            ytrain = np.delete(ytrain, 0); ytrain = np.delete(ytrain, -1)\n","        xval = np.asarray(xval)\n","        yval = np.asarray(yval)\n","        print( '{} elements in validation set'.format(len(yval)) )\n","\n","        yval = np.reshape(yval, newshape=(yval.shape[0], 1))\n","        ytrain = np.reshape(ytrain, newshape=(ytrain.shape[0], 1))\n","        \n","        labels = tf.placeholder(tf.float32, shape=(None, 1), name='train_labels')\n","        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='train_batch')\n","        ul_batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='ul_batch')\n","        \n","        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n","\n","\n","        y_true=labels\n","        y_pred=self(batch)[0]\n","\n","        true_positives = b.sum(b.round(b.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = b.sum(b.round(b.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + b.epsilon())\n","        true_positives = b.sum(b.round(b.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = b.sum(b.round(b.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + b.epsilon())\n","        fscore= 2*((precision*recall)/(precision+recall+b.epsilon()))\n","\n","        loss, emb = self.get_loss(batch, labels)\n","        if (loss_type == 'adv'):\n","            loss += self.get_adv_loss(batch, labels, loss, emb, p_mult)\n","        elif (loss_type == 'v_adv'):\n","            loss += self.get_v_adv_loss(ul_batch, p_mult)\n","\n","        opt = self.optimizer.minimize( loss )\n","\n","        # Initialiser parametres\n","        if (init is None):\n","            self.sess.run( [var.initializer for var in tf.global_variables() if not('embedding' in var.name)] )\n","            print( 'Random initialization' )\n","        else:\n","            saver = tf.train.Saver()\n","            saver.restore(self.sess, init)\n","            print( 'Restored value' )\n","        \n","        _losses = list()\n","        _accuracies = list()\n","        _fscores = list()\n","        list_ratio = (len(ultrain)/len(ytrain)) if (ultrain is not None) else None\n","\n","\n","\n","        # Boucle d'iter\n","        for epoch in range(epochs):\n","            losses = list()\n","            accuracies = list()\n","            validation = list()\n","            fscores = list()\n","\n","            bar = ProgressBar(max_value=np.floor(len(ytrain)/batch_shape[0]).astype('i'))\n","            minibatch = enumerate(self.get_minibatch(xtrain, ytrain, ultrain, batch_shape=batch_shape))\n","            for i, train_batch in minibatch:\n","                fd = {batch: train_batch['x'], labels: train_batch['y'], K.backend.learning_phase(): 1} #training mode\n","                if (loss_type == 'v_adv'):\n","                    fd.update( {ul_batch: train_batch['ul']} )\n","                \n","                _, acc_val, loss_val = self.sess.run([opt, accuracy, loss], feed_dict=fd)\n","                accuracies.append( acc_val )\n","                losses.append( loss_val )\n","                fscores.append(self.sess.run(fscore, feed_dict=fd))\n","\n","                bar.update(i)\n","            \n","            # Sauvegarde metrics\n","            _accuracies.append( accuracies )\n","            _losses.append(losses)\n","            logfile = savepath+'lstm_adv'+trait+'.log'\n","            log_msg = \"\\nEpoch {} of {} -- average accuracy is {:.3f} (train) -- average loss is {:.3f} -- average f1score is {:.3f}\"\n","            results = open(logfile,\"a\")\n","            results.write('\\n\\n')\n","            results.write(log_msg.format(epoch+1, epochs, np.asarray(accuracies).mean(), np.asarray(losses).mean(), np.asarray(fscores).mean())+'\\n\\n')\n","            results.close()\n","            \n","            log_msg = \"\\nEpoch {} of {} -- average accuracy is {:.3f} (train) -- average loss is {:.3f} -- average f1score is {:.3f}\"\n","            print( log_msg.format(epoch+1, epochs, np.asarray(accuracies).mean(), np.asarray(losses).mean(), np.asarray(fscores).mean()) )\n","            \n","            # validation\n","            self.validation(xval, yval, trait, batch_shape=batch_shape)\n","            \n","        ##################################\n","\n","        #       Save Model\n","\n","        ##################################\n","\n","        if saving :\n","              print('saving model')\n","              dict_shape = dict_weight.shape\n","              clone_model = K.Sequential (\n","                  [\n","                   K.layers.Embedding(dict_shape[0], dict_shape[1], weights=[dict_weight], trainable=False, name='embedding'),\n","                   K.layers.Dropout(rate=dropout, seed=91, name='dropout'),\n","                   K.layers.LSTM(lstm_units, stateful=False, return_sequences=False, name='lstm'),\n","                   K.layers.Dense(dense_units, activation='relu', name='dense'),\n","                   K.layers.Dense(1, activation='sigmoid', name='p')\n","                  ]\n","              )\n","              clone_model.layers[0].set_weights(self.emb.get_weights())\n","              clone_model.layers[1].set_weights(self.drop.get_weights())\n","              clone_model.layers[2].set_weights(self.lstm.get_weights())\n","              clone_model.layers[3].set_weights(self.dense.get_weights())\n","              clone_model.layers[4].set_weights(self.p.get_weights())\n","              clone_model.save(savepath+'lstm_adv'+trait+'.h5')\n","\n","        \n","        # Graphe d'erreur et acc\n","        plt.plot([np.asarray(l).mean() for l in _losses], color='red', linestyle='solid', marker='o', linewidth=2)\n","        plt.plot([np.asarray(a).mean() for a in _accuracies], color='blue', linestyle='solid', marker='o', linewidth=2)\n","        plt.savefig(savepath+'train_{}_e{}_m{}_l{}.png'.format(loss_type, epochs, batch_shape[0], batch_shape[1]))\n","\n","        ##################################\n","\n","        #        Test\n","\n","        ##################################\n","\n","    def test(self, dataset, trait ,batch_shape=(256, 400)):\n","        print( 'Test...' )\n","        xtest = np.load( \"{}\".format(dataset)+trait+\"_xtest.npy\" ,allow_pickle=True)\n","        ytest = np.load( \"{}\".format(dataset)+trait+\"_ytest.npy\",allow_pickle=True )\n","        ytest = np.reshape(ytest, newshape=(ytest.shape[0], 1))\n","        \n","        labels = tf.placeholder(tf.float32, shape=(None, 1), name='test_labels')\n","        batch = tf.placeholder(tf.float32, shape=(None, batch_shape[1]), name='test_batch')\n","\n","        accuracy = tf.reduce_mean( K.metrics.binary_accuracy(labels, self(batch)[0]) )\n","        y_true=labels\n","        y_pred=self(batch)[0]\n","\n","        true_positives = b.sum(b.round(b.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = b.sum(b.round(b.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + b.epsilon())\n","        \n","        \n","        true_positives = b.sum(b.round(b.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = b.sum(b.round(b.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + b.epsilon())\n","        \n","        \n","        fscore= 2*((precision*recall)/(precision+recall+b.epsilon()))\n","\n","\n","        accuracies = list()\n","        fscores=list()\n","        recalls=list()\n","        precisions=list()\n","        bar = ProgressBar(max_value=np.floor(len(ytest)/batch_shape[0]).astype('i'))\n","        minibatch = enumerate(self.get_minibatch(xtest, ytest, ul=None, batch_shape=batch_shape))\n","        for i, test_batch in minibatch:\n","            fd = {batch: test_batch['x'], labels: test_batch['y'], K.backend.learning_phase(): 0} #test mode\n","            accuracies.append( self.sess.run(accuracy, feed_dict=fd) )\n","            fscores.append(self.sess.run(fscore, feed_dict=fd))\n","            recalls.append(self.sess.run(recall, feed_dict=fd))\n","            precisions.append(self.sess.run(precision, feed_dict=fd))\n","            bar.update(i)\n","\n","        # Metrics\n","        logfile = savepath+'lstm_adv'+trait+'.log'\n","        print( \"\\nAverage accuracy is {:.3f}\".format(np.asarray(accuracies).mean()) )\n","        print( \"\\nF1-scores is \",fscores )\n","        print( \"\\nRecalls is \",recalls )\n","        print( \"\\nPrecisions is \",precisions )\n","        results = open(logfile,\"a\")\n","        results.write('\\n')\n","        results.write('----------------------------------------TEST-------------------------------\\n')\n","        results.write('Test score:  '+str(np.asarray(fscores).mean())+'\\n\\n')\n","        results.write('Test acc:  '+str(np.asarray(accuracies).mean())+'\\n\\n')\n","        results.close()\n","        \n","        \n","        \n","def main(data, n_epochs, n_ex, ex_len, lt, pm):\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n","    config = tf.ConfigProto(log_device_placement=True)\n","    config.gpu_options.allow_growth = True\n","    session = tf.Session(config=config)\n","\n","    embedding_weights = np.load( \"{}embedding_matrix_200d.npy\".format(data),allow_pickle=True )\n","    trait = 'CON'\n","    net = Network(session, embedding_weights, lr=0.0001, dropout=0.2, lstm_units=1024, dense_units=30)\n","    net.train(data, trait ,embedding_weights , batch_shape=(n_ex, ex_len), \n","              epochs=25, loss_type=lt, p_mult=pm, init=None, save=None, \n","              dropout=0.2, lstm_units=1024, dense_units=30,saving=True)\n","    net.test(data, trait, batch_shape=(n_ex, ex_len))\n","    K.backend.clear_session()\n","\n","\n","\n","main(data='gdrive/My Drive/AI-project/Embedding', n_epochs=20, n_ex=512, ex_len=30, lt='v_adv', pm=0.02)"],"execution_count":null,"outputs":[]}]}